% 导言区
\documentclass{ctexart}

\usepackage{etoolbox}

\usepackage{amsmath}
\usepackage{amssymb}

% 花体宏包
\usepackage[mathscr]{euscript}

\usepackage{enumerate}

% \ctexset{secnumdepth=4, tocdepth=3}
\setcounter{tocdepth}{3}    % toc即table of content，表示目录显示的深度
\setcounter{secnumdepth}{4} % secnum即section number，表示章节编号的深度
% -1 part
% 0 chapter
% 1 section
% 2 subsection
% 3 subsubsection
% 4 paragraph
% 5 subparagraph


% 公式按章节编号
\numberwithin{equation}{section}

% 调整页边距
\usepackage{geometry}
\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% 代码块
\usepackage{listings}

% 调整代码块行距
\AtBeginEnvironment{lstlisting}{\linespread{1.0}}

% 高亮使用的颜色
\usepackage{xcolor} 
\definecolor{commentcolor}{RGB}{85,139,78}
\definecolor{stringcolor}{RGB}{206,145,108}
\definecolor{keywordcolor}{RGB}{34,34,250}
\definecolor{backcolor}{RGB}{245,245,245}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
	frame             = tb,  % 显示边框:lrtb;大写表示双线
	language          = Python,
	aboveskip         = 3mm,
	belowskip         = 3mm,
	showstringspaces  = false, % 不显示字符串中的空格
	% linewidth         = 1. \linewidth % 设置行宽
	xleftmargin       = 0em,   % 表格框 整体距左侧边线的距离为 2em
	xrightmargin      = 0em,   
	% framexleftmargin  = 0em,
	% framexrightmargin = 0em,
	framextopmargin   = 0pt,
	columns           = fixed, % 行距离:flexible,灵活;
	basicstyle        = {\small\ttfamily},
	numbers           = left,  % 行号的位置:left,左侧;
	numbersep         = 8pt,  %设置行号与代码的距离，默认是5pt
	numberstyle       = \tiny\color{gray},
	keywordstyle      = \color{blue},
	commentstyle      = \color{dkgreen},
	stringstyle       = \color{mauve},
	backgroundcolor   = \color{backcolor}, % 背景颜色
	breaklines        = true,   % 自动换行
	breakatwhitespace = true,   
	% breakindent       = 4em, %
	tabsize           = 4,
	extendedchars     = false   % 解决代码跨页时，章节标题，页眉等汉字不显示的问题  
}


% 信息
\title{树模型 \\ Tree Models}
\author{VEAGER}
\date{\today}




% 正文区
\begin{document}
	
\maketitle

% 换页
\newpage

% 第一节 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 第一节
\section{决策树 Decision Tree}

% 1.1 小节
\subsection{分裂指标}

% 1.1.1 小节
\subsubsection{离散特征分裂指标}

\begin{lstlisting} % 代码定格写，不然也会被排版
	
import os
import numpy as np
\end{lstlisting}

在 \emph{分类任务} 中，定义数据集 $D = \left\{ \boldsymbol{x_n}, y_n \right\} ^{N}_{n=1}  $ ，$ N = \left| D \right|$ 表示样本容量（样本总数）；$x_n \in \mathbb{R}^P $ 为输入样本，样本维度为 $P$ ；$y_n \in \mathbb{R} $ 为输出样本，样本维度为 $1$ ；特征集 $\mathbb{A} = \left\{ A_1, A_2, \cdots, A_p, \cdots, A_P \right\}$。

设输出样本 $y_n,n=1,2,...,N$ 有 $K$ 个类，每一类的的集合为 $ C_k $，其样本数量为 $ \left| C_k \right|$ ，则有：
\begin{align}
	D &= \bigcup^{K}_{k=1} C_k \\
	N = \left| D \right| &= \sum^{K}_{k=1} \left| C_k \right|
\end{align}

对于某一特征 $A_p ( p=1,2,...,P)$（简写为 $A$），设特征 $A$ 有 $M$ 个不同的取值 $\left\{ a_1,a_2,...,a_M \right\}$，根据特征 $A$ 的取值将数据集 $D$ 划分为 $M$ 个子集 $D_1,D_2,...,D_M$，则有：
\begin{align}
	D &= \bigcup^{M}_{m=1} D_m \\
	N = \left| D \right| &= \sum^{M}_{m=1} \left| D_m \right|
\end{align}

记划分子集 $D_m$ 中，属于类 $C_k$ 的样本的合集为 $D_{mk}$，即：
\begin{align}
	D_{mk} &= D_m \cap C_k \\
	D_m  &= \bigcup^{K}_{k=1} D_{mk} \\
	\left| D_m \right| &= \sum^{K}_{k=1}  \left| D_{mk} \right|
\end{align}


% 1.1.1.1
\paragraph{信息熵}~{}

在信息论与概率统计中，\emph{熵（entropy）}是表示随机变量不确定的度量。

设 $X$ 是一个取有限个值的离散随机变量，其概率分布为：
\begin{equation}
	P(X=x_i)=p_i, \quad i=1,2,...,n
\end{equation}

则随机变量 $X$ 的熵定义为：
\begin{equation}
	H(X) = H(p) = -\sum_{i=1}^{n} p_i \log p_i \label{eq-entropy}
\end{equation}

由定义可知，熵只依赖于 $X$ 的分布，而与 $X$ 的取值无关，所以也可将 $X$ 的熵记作 $H(p)$ 。在式 \ref{eq-entropy} 中，若 $p_i=0$，则定义 $0 \log 0 = 0$。通常，式 \ref{eq-entropy} 中的对数以 $2$ 为底或以 $e$ 为底（自然对数），这时熵的单位分别称作比特（bit）或纳特（nat）。

熵越大，随机变量的不确定性就越大。从定义可验证：
\begin{equation}
	0 \leq H(p) \leq \log n
\end{equation}

设有随机变量 $(X,Y)$ ，其联合概率分布 $P(X=x_i,Y=y_j)$ 为：
\begin{equation}
	P(X=x_i,Y=y_j) = p_{ij}, \quad i=1,2,...,n; \quad j=1,2,...,m
\end{equation}

随机变量 $X$ 的边际分布 $P(X)$ 为：
\begin{equation}
	P(X=x_i) = p_i = p_{i \cdot} = \sum_{j=1}^{m} p_{ij}, \quad i=1,2,...,n
\end{equation}

\emph{条件熵（conditional entropy）} $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性，定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望：
\begin{equation}
	H(Y|X) = -\sum_{i=1}^{n} p_i H(Y|X=x_i)
\end{equation}

当 \emph{熵} 和 \emph{条件熵} 中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为\emph{经验熵（empirical entropy） }和\emph{经验条件熵（empirical conditional entropy）}。此时，如果由 $0$ 概率，令 $ 0 \log 0 = 0 $ 。


% 1.1.1.2
\paragraph{信息增益}~{}

\emph{信息增益（information gain）}表示得知特征 $X$ 的信息而使得类 $Y$ 得信息的不确定性减少的程度。对于数据集 $D$ ，特征 $A$ 的信息增益 $g(D,A)$ ，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即：
\begin{equation}
	g(D,A) = H(D) - H(D|A)
\end{equation}

一般地，熵 $H(Y)$ 与条件熵  $H(Y|X)$ 之差为 \emph{互信息（mutual information）}。

计算特征 $A$ 对数据集 $D$ 的信息增益 $g(D,A)$ 包括以下三个步骤：

\textbf{步骤1：}计算数据集 $D$ 的熵 $H(D)$ ：
	\begin{equation}
		H(D) = -\sum_{k=1}^{K} \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}
	\end{equation}

\textbf{步骤2：}计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D|A)$ ：
	\begin{equation}
		\begin{split}
			H(D|A) &= \sum_{m=1}^{M} \frac{|D_m|}{|D|} H(D_m) \\
	               &= -\sum_{m=1}^{M} \frac{|D_m|}{|D|} \sum_{k=1}^K \frac{|D_{mk}|}{|D_m|} \log_2 \frac{|D_{mk}|}{|D_m|}
		\end{split}
	\end{equation}

\textbf{步骤3：}计算信息增益 $g(D,A)$ ：
	\begin{equation}
		g(D,A) = H(D) - H(D|A)
	\end{equation}


% 1.1.1.3
\paragraph{信息增益比}~{}

\emph{信息增益比（information gain ratio）}，也称为\emph{信息增益率}：特征 $A$ 对数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即：
\begin{equation}
	g_R(D,A) = \frac{g(D,A)}{H_A(D)}
\end{equation}

上式中， $H_A(D)$ 表示数据集 $D$ 关于特征 $A$ 的值的熵，计算公式如下：
\begin{equation}
	H_A(D) = -\sum_{m=1}^{M} \frac{|D_m|}{|D|} \log_2 \frac{|D_m|}{|D|}
\end{equation}


% 1.1.1.4
\paragraph{基尼指数}~{}

\emph{基尼指数（Gini index）} ，也被称为\emph{基尼不纯度（Gini impurity）}。用于衡量数据集 $D$ 的\emph{不纯度（impurity）}。直观来说，基尼指数反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。基尼指数越小，表明数据集 $D$ 的纯度越高（不纯度越低），样本的不确定性也就越小，这与\emph{熵}相似。计算公式如下：
\begin{equation}
	\begin{split}
		\text{Gini}(p) &= \sum_{k=1}^K p_k (1 - p_k) \\
					   &= 1 - \sum_{k=1}^K p_k^2
	\end{split}
\end{equation}

对于数据集 $D$，其基尼指数为：
\begin{equation}
	\text{Gini}(D) = 1 - \sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right)^2
\end{equation}

在特征 $A$ 的条件下，集合 $D$ 的基尼指数 $\text{Gini}(D,A)$ 定义为：
\begin{equation}
	\text{Gini}(D,A) = \sum_{m=1}^M \frac{|D_m|}{|D|} \text{Gini}(D_m)
\end{equation}

基尼指数一般用于\textbf{CART 算法}完成\emph{分类}任务，并且采用二分法分裂。因此，数据集根据特征 $A$ 是否取某一值 $a$ 被划分为 $D_1$ 和 $D_2$ 两个部分，即：
\begin{subequations}
	\begin{align}
		D_1 &= \left\{(\boldsymbol{x},y) \in D | A(\boldsymbol{x}) = a \right\} \\
		D_2 &= D - D_1
	\end{align}
\end{subequations}


此时，在特征 $A$ 的条件下，集合 $D$ 的基尼指数 $\text{Gini}(D,A)$ 定义为：
\begin{equation}
	\text{Gini}(D,A) = \frac{|D_1|}{|D|} \text{Gini}(D_1) + \frac{|D_2|}{|D|} \text{Gini}(D_2)
\end{equation}


% 1.1.2 小节
\subsubsection{连续特征分裂指标}

由于决策树一般要求特征变量为连续变量，因此，在数据预处理阶段，需要使用\emph{连续数离散化技术}对连续变量进行预处理。

在决策树模型中，\textbf{C4.5算法}使用\emph{二分法（bi-partition）}对连续数据离散化处理。

对于数据集 $D$ 和某一特征的连续特征 $A$，假设特征 $A$ 在数据集 $D$ 上有 $N$ 个值（即有 $N = |D|$ 个样本）。首先，将这些值从小到大进行排序，得到 $\left\{ a_1, a_2, \cdots, a_N \right\}$ 。 对于某一划分点 $ t(a_1 \leq t < a_N)$，可以将数据集 $D$ 分成两个子集：
\begin{subequations}
	\begin{align}
		D_1 &= \left\{(\boldsymbol{x},y) \in D | A(\boldsymbol{x}) \leq t \right\} \\
		D_2 &= \left\{(\boldsymbol{x},y) \in D | A(\boldsymbol{x}) > t \right\}
	\end{align}
\end{subequations}

其中，对于相邻的特征取值 $a_n$ 和 $a_{n+1}$，$t$ 在区间 $[a_n, a_{n+1})$ 中取任意值所产生的划分结果相同。因此，在实际的操作中，切分点 $t$ 往往取区间下界 $a_n$ 或 区间中点 $(a_n, a_{n+1}) / 2$。

进而，连续特征 $A$ 被转换成二值化的离散特征，从而根据离散特征计算分裂指标。

\subsubsection{回归问题}

\textbf{CART 算法} 可以用于实现\emph{回归}任务。在\emph{回归}任务中，每个叶子结点的值 $c_m$ 为落入该结点样本的平均值，即：
\begin{equation}
	c_m = \frac{1}{|R_m|} \sum_{(\boldsymbol{x}_i, y_i) \in R_m} y_i
\end{equation}

上式中，$R_m$ 为落入第 $m$ 个叶子结点的样本合集。


% 1.1.2.1
\paragraph{均方误差（MSE）}~{}

用于\emph{回归}任务的决策树，对于每个集合 $D_m$ ，其MSE度量指标 $H(D_m)$ 的计算公式为：
\begin{equation}
	H(D_m) = \frac{1}{|D_m|} \sum_{(\boldsymbol{x}_i, y_i) \in D_m} (y_i - c_m)^2
\end{equation}

上式中，$c_m$ 为数据子集的 $D_m$ 的均值： 
\begin{equation}
	c_m = \frac{1}{|D_m|} \sum_{(\boldsymbol{x}_i, y_i) \in D_m} y_i
\end{equation}

可以看出，MSE的度量指标 $H(D_m)$ 实际上为集合 $D_m$ 的方差，即：
\begin{equation}
	H(D_m) = \mathop{\text{VAR}}\limits_{(\boldsymbol{x}_i, y_i) \in D_m} y_i
\end{equation}

% 1.1.3.1
\paragraph{改进的均方误差（Friedman MSE）}~{}




% 1.1.4.1
\paragraph{平方绝对误差（MAE）}~{}

\begin{equation}
	H(D_m) = \frac{1}{|D_m|} \sum_{(\boldsymbol{x}_i, y_i) \in D_m} |y_i - c_m|
\end{equation}


% 1.1.5.1
\paragraph{Half Poisson Deviance}~{}

\begin{equation}
	H(D_m) = \frac{1}{|D_m|} \sum_{(\boldsymbol{x}_i, y_i) \in D_m} \left( y_i \log \frac{y_i}{c_m} - y_i + c_m \right)
\end{equation}



\begin{equation}
	\text{MSE}(D,A,t) = \frac{1}{|D_1|} \sum_{(\boldsymbol{x}_i, y_i) \in D_1} (y_i-c_1)^2 + \frac{1}{|D_2|} \sum_{(\boldsymbol{x}_j, y_j) \in D_2} (y_j-c_2)^2
\end{equation}

在上式中，$c_1$ 和 $c_2$ 分别为数据子集 $D_1$ 和 $D_2$ 的均值：
\begin{subequations}
	\begin{align}
		c_1 &= \frac{1}{|D_1|} \sum_{(\boldsymbol{x}_i, y_i) \in D_1} y_i \\
		c_2 &= \frac{1}{|D_2|} \sum_{(\boldsymbol{x}_i, y_j) \in D_2} y_j
	\end{align}
\end{subequations}

\begin{equation}
	\text{MAE}(D,A,t) = \frac{1}{|D_1|} \sum_{(\boldsymbol{x}_i, y_i) \in D_1} |y_i-c_1| + \frac{1}{|D_2|} \sum_{(\boldsymbol{x}_j, y_j) \in D_2} |y_j-c_2|
\end{equation}

在上式中，$c_1$ 和 $c_2$ 分别为数据子集 $D_1$ 和 $D_2$ 的中位数：
\begin{subequations}
	\begin{align}
		c_1 &= \frac{1}{|D_1|} \text{median} \{y_i | (\boldsymbol{x}_i, y_i) \in D_1 \} \\
		c_2 &= \frac{1}{|D_2|} \text{median} \{y_j | (\boldsymbol{x}_j, y_j) \in D_2 \}
	\end{align}
\end{subequations}


\subsubsection{代码实现}

\paragraph{信息熵}~{}




% 1.2 
\subsection{ID3}

% 1.2.1
\subsubsection{数学原理}

\emph{ID3算法} 是以 \emph{信息增益} 为准则来划分属性。

\textbf{ID3算法流程：}
 
\textbf{输 入：} 数据集 $D = \left\{ \boldsymbol{x_n}, y_n \right\} ^{N}_{n=1}$ ，$x_n \in \mathbb{R}^P$ ，$y_n \in \mathbb{R} $ ；特征集 $\mathbb{A} = \left\{ A_1, A_2, \cdots, A_p, \cdots, A_P \right\}$ ；划分阈值 $\varepsilon$。

\textbf{输 出：} 决策树 $\mathscr{T}$

\textbf{步骤 1：} 若数据集 $D$ 中所有样本属于同一类 $C_k$，并将类 $C_k$ 作为该结点的类标记，返回 $\mathscr{T}$。

\textbf{步骤 2：} 选择划分属性 $A^*$ ：
\begin{enumerate}[\qquad ]
	\item (1) 特征集 $\mathbb{A} = \varnothing$，则 $\mathscr{T}$ 为单节点树，并将类 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $\mathscr{T}$。
	
	\item (2) 否则，计算各特征 $A_p$ 对 $D$ 的 \emph{信息增益} $g(D,A_p)$，选择 \emph{信息增益} 最大的特征 $A^*$，即：
	\begin{equation}
		A^* = \mathop{\arg\max}\limits_{A_p \in \mathbb{A}} g(D,A_p)
	\end{equation}
\end{enumerate}


\textbf{步骤 \ \ 3：} 生成叶子结点：
\begin{enumerate}[\qquad ]
	\item (1) 若 $A^{*}$ 的 \emph{信息增益} 小于阈值 $\varepsilon$，则将数据集 $D$ 中样本数最大的类 $C_k$ 作为该结点的类标记，返回 $\mathscr{T}$。
	
	\item (2) 否则，对 $A^{*}$ 的每一取值 $a_i$，根据 $A^* = a_i$ 将数据集 $D$ 分割为若干非空子集 $D_i$，将每一子集 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $\mathscr{T}$，返回 $\mathscr{T}$。
\end{enumerate}

\textbf{步骤 \ \ 4：}对第 $i$ 个子结点，以 $D_i$ 为数据集，以 $\mathbb{A} - \left\{ A^* \right\}$ 为特征集，递归地调用 \textbf{步骤1} $\sim$ \textbf{步骤3}，得到子树 $\mathscr{T}_i$，返回 $\mathscr{T}_i$。


% 1.2.2
\subsubsection{代码实现}

% 1.2.3
\subsubsection{实例结果}


% 1.2.
\subsection{C4.5}

% 1.2.1
\subsubsection{数学原理}

\textbf{C4.5算法}与\textbf{ID3算法}过程相似，唯一的区别在于\textbf{C4.5算法}是以\textbf{信息增益比}为准则来划分属性。\textbf{信息增益}准则对可取值数目较多得属性有所偏好，使用\textbf{信息增益比}可以较少这种偏好带来得不利影响。

% 1.2.2
\subsubsection{代码实现}

% 1.2.3
\subsubsection{实例结果}


% 1.3.
\subsection{分类回归树 CART}

\textbf{分类与回归树（classification and regression tree，CART）} 即可用于 \emph{分类} 任务，又可用于\emph{回归}任务。\textbf{CART} 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。因此，相比于上述两种的决策树模型，对于\textbf{CART算法}，在叶子结点每次分裂（树成长）的过程中，不仅需要确定最优划分特征，还需要确定最优划分特征的划分取值，对于\emph{分类}任务和\emph{回归}任务均是如此。

% 1.3.1
\subsubsection{分类问题}

% 1.3.2
\subsubsection{回归问题}

\textbf{CART算法}也可以实现回归任务，用于实现回归任务的决策树，也被称为\emph{回归树（regression tree）}。在\textbf{CART算法}中，树模型被要求为是\emph{二叉树}结构，因此分裂指标的为左右叶子结点的指标之和。

例如，若采用MSE分裂指标，则分裂指标的计算方法如式\ref{eq-reg-mse} 所示。这样的\emph{回归树}通常被称为\emph{最小二乘回归树（least squares regression tree）}。
\begin{equation}
	H(D,A,t) = \frac{1}{|D_1|} \sum_{(\boldsymbol{x}_i, y_i) \in D_1} (y_i-c_1)^2 + \frac{1}{|D_2|} \sum_{(\boldsymbol{x}_j, y_j) \in D_2} (y_j-c_2)^2 \label{eq-reg-mse}
\end{equation}

\textbf{CART回归树流程：}

\textbf{输 入：} 数据集 $D = \left\{ \boldsymbol{x_n}, y_n \right\} ^{N}_{n=1}$ ，$x_n \in \mathbb{R}^P$ ，$y_n \in \mathbb{R} $ ；特征集 $\mathbb{A} = \left\{ A_1, A_2, \cdots, A_p, \cdots, A_P \right\}$ ；划分阈值 $\varepsilon$。

\textbf{输 出：} 决策树 $\mathscr{T}$。

在数据集所在的输入空间种，递归地将每个区域划分为两个子区域并决定每个子区域地输出值，构建二叉决策树：

\textbf{步骤1：} 寻找最优划分特征 $A^*$ 及其划分值 $a^*$。
\begin{enumerate}[\qquad ]
	\item (1) 确定每个特征 $A_p \in \mathbb{A}$ 的最优划分值 $a^*$。对于每个特征 $A_p$，遍历其所有的取值，选择使得划分指标 $H(D,A,a)$ 或 $H(D,A,t)$ 最小的特征值 $a^*$ 或 $t^*$ 。注意，离散特征和连续特征的特征值有所差异。
	\begin{equation}
		a^* = \mathop{\arg\min}\limits_{a \in \{a_1,a_2,...\}} H(D,A_p,a)
	\end{equation}
	
	\item (2) 确定最优划分特征 $A^*$。根据每个特征 $A_p$ 所计算得到的最优划分指标 $H(D, A_p)$，选择最优的划分特征，即：
	\begin{equation}
		A^* = \mathop{\arg\max}\limits_{A_p \in \mathbb{A}} H(D,A_p)
	\end{equation}
\end{enumerate}

\textbf{步骤2：} 使用选定的划分特征 $A^*$ 和划分值 $a^*$ 划分区域 $R_m$ 并决定相应的输出值 $c_m$：
\begin{subequations}
	\begin{align}
		R_1(A^*,a^*) = \left\{(\boldsymbol{x},y) \in D | A(\boldsymbol{x}) = a^* \right\}, \quad 
		c_1 = \frac{1}{|R_1|} \sum_{(\boldsymbol{x}_i,y_i) \in R_1} y_i \\
		R_2(A^*,a^*) = \left\{(\boldsymbol{x},y) \in D | A(\boldsymbol{x}) \neq a^* \right\}, \quad
		c_2 = \frac{1}{|R_2|} \sum_{(\boldsymbol{x}_i,y_i) \in R_2} y_i
	\end{align}
\end{subequations}

\textbf{步骤3：} 对于叶子结点 $R_1$ 和 $R_2$ 递归地调用\textbf{步骤1} $\sim$ \textbf{步骤2}，直到满足终止条件。

\textbf{步骤4：} 最终，数据集被划分为 $M$ 个区域 $R_m(m=1,2,\cdots,M)$，每个区域（叶子结点）的输出为落入该区域样本的平均值。因此，可以得到决策树最终的输出为：
\begin{equation}
	f(\boldsymbol{x}) = \sum_{m=1}^M c_m \text{I} \left( (\boldsymbol{x},y) \in R_m \right)
\end{equation}

上式中，$\text{I}(x)$ 为\emph{指数函数}，当括号内 $x$ 为真，函数返回 1，否则返回0。

% 1.3.3
\subsubsection{实例结果}


\subsection{泛化技术}

\subsubsection{预剪枝}

\subsubsection{后剪枝}

\subsection{连续值与缺失值处理}


	
\end{document}